\documentclass[11pt]{article}

\usepackage[top=50mm, bottom=50mm, left=50mm, right=50mm]{geometry}
%\usepackage[a4paper,top=2.5cm,bottom=2.5cm,left=2.5cm,right=2.5cm,marginparwidth=1.75cm]{geometry}
% for adding numbers for lines 
\usepackage{lineno}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{float}
\usepackage{multirow}
\usepackage{color}
\usepackage{lineno}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage[normalem]{ulem} 
\usepackage{makeidx}
\usepackage{xspace}
\usepackage{wrapfig}
\usepackage{newfloat}
\makeindex

\DeclareFloatingEnvironment[name={Supplementary Figure},fileext=lof]{suppfigure}


\newtheorem{theorem}{Theorem}
\newtheorem{Definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{Theorem}{Theorem}
\newtheorem{Lemma}{Lemma}
\newtheorem{Claim}{Claim}
\newtheorem{Notation}{Notation}
\newtheorem{Algorithm}{Algorithm}
\newtheorem{Observation}{Observation}


%%%%%%%%%%%%%%% macros by yusu %%%%%%%%%%%%%
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{obs}{Observation}

\definecolor{darkred}{rgb}{1, 0.1, 0.3}
\definecolor{darkblue}{rgb}{0.1, 0.1, 1}
\definecolor{darkgreen}{rgb}{0,0.6,0.5}
\newcommand{\yusu}[1]		{{ \textcolor{darkblue} {{\sc Yusu Says:} #1}}}

\newcommand {\mm}[1] {\ifmmode{#1}\else{\mbox{\(#1\)}}\fi}
\newcommand{\denselist}{\itemsep 0pt\parsep=1pt\partopsep 0pt}

\newcommand{\etal}		{{et al.}\xspace}
\newcommand{\myeg}	{{e.g.}\xspace}

\newcommand{\eps}		{\varepsilon}
\newcommand{\myanceq}	{{\succeq}} %ancerster 
\newcommand{\myanc}	{{\succ}} %ancerster 
\newcommand{\mydesceq} {{\preceq}}
\newcommand{\mydesc}		{{\prec}}
\newcommand{\dgh}		{\delta_{\mathcal{GH}}} %GH distance
\newcommand{\optdI}	{\mu}
\newcommand{\myeps}	{\eps}
\newcommand{\mygood}		{{good}\xspace}
\newcommand{\myk}		{\tau} %{\kappa}
\newcommand{\mydelta}	{\delta}
\newcommand{\nTone}	{{\widehat{T}_{1, \mydelta}^{\hat{f}}}}
\newcommand{\nTtwo}	{{\widehat{T}_{2, \mydelta}^{\hat{g}}}}
\newcommand{\Img}		{{\mathrm{Im}}}
\newcommand{\myL}		{{L}}
\newcommand{\superlevel}		{{super-level}\xspace}
\newcommand{\myC}		{{\mathrm{C}}}
\newcommand{\setSL}	{{\mathcal{L}}}
\newcommand{\slone}[1] 	{{\mathrm{L}^{(1)}_{#1}}}
\newcommand{\sltwo}[1] 	{{\mathrm{L}^{(2)}_{#1}}}
\newcommand{\hatT}		{{\widehat{T}}}
\newcommand{\levelC}		{\mathrm{Ch}} %{{levelC}}
\newcommand{\LCA}		{{LCA}}%common ancester
\newcommand{\DPalg}	{{\sf DPgoodmap}}
\newcommand{\modifyDPalg}	{{\sf modified-DP}}
\newcommand{\myF}		{{\mathcal{F}}}
\newcommand{\mypartial}[1]		{{partial-{#1}-good}\xspace}
\newcommand{\myvalue}		{\mathrm{value}}
\newcommand{\degbound}		{{degree-bound}\xspace}
\newcommand{\mydepth}		{{depth}\xspace}
\newcommand{\ep}		{{\varepsilon_p}}

\newcommand{\Intdopt}		{{\mydelta^*}}
\newcommand{\criSet}		{{\Pi}}
\newcommand{\Tcal}			{\mathcal{T}}
\newcommand{\mycost}		{{\mathrm{cost}}}
\newcommand{\uTone}		{{|T_1^f|}}
\newcommand{\uTwo}		{{|T_2^g|}}
\newcommand{\uT}				{{|T|}}
\newcommand{\mywF}		{{w^F}}
\newcommand{\mykpara}	{{\myk_\mydelta}}
\newcommand{\newdepth}	{{depth}}
\newcommand{\newtau}	{{\widehat{\tau}}}

\newcommand{\sensiblepair}     {{sensible-pair}\xspace}
\newcommand{\Edgelistpair}      {{Edge-list pair}\xspace}
\newcommand{\edgelistpair}      {{edge-list pair}\xspace}
\newcommand{\Fnew}          {{F_{new}}}
\newcommand{\Fold}          {{F_{old}}}

%%%%%%%%%%%%%% macros by yusu end %%%%%%%%%%%%%

\begin{document}

\title{Monitoring Health Systems by Estimating Excess Mortality}
 
\author{
{Rolando J. Acosta \footnote{Department of Biostatistics, Harvard University, Cambridge, MA, USA}} \and {Rafael A. Irizarry \footnote{Department of Data Science, Dana-Farber Cancer Institute; \newline Department of Biostatistics, Harvard University, Cambridge, MA, USA}}
}

%\begin{document}
\maketitle
%\linenumbers
\setcounter{page}{0}

\begin{abstract}
Monitoring health systems during and after natural disasters, epidemics, or outbreaks is paramount to guiding policy decisions and interventions. In the case of natural disaster the effects of these events on public health can be \textit{direct} or \textit{indirect}. Direct effects are defined as those resulting from the immediate destruction such as drowning and trauma from flying debris, while indirect effects are delayed, longer lasting, and, often, harder to measure or detect. In the case of outbreaks and epidemics, lack of comprehensive testing or reporting can lead to challenges in measuring direct effects, while indirect effects can arise due to, for example, increased stress levels or reduced access to health services. When the effects are long lasting and difficult to detect, in the short term, the accumulated effects can actually be devastating. Improved access to mortality data provides an opportunity to develop data-driven approaches that can help monitor health systems.
Here we describe a statistical methodology and software that facilitates data-driven approaches. Our work was motivated by events occurring in Puerto Rico after the passage of Hurricane Mar\'ia, but can be applied in other contexts such as estimating the effects of an epidemic in the presence of inaccurate reporting of cases. We make our tools available as a free and open source R package and an interactive webtool.
\end{abstract}

\newpage

\section{Introduction}
\label{sec:introduction}
Hurricane Mar\'ia made landfall in Puerto Rico on September 20, 2017, interrupting the water supply, electricity, telecommunications networks, and access to medical care\cite{updates2018puerto, rogers2017puerto}. Two weeks after the hurricane the official death count stood at 16\cite{hoyos2018puerto}. This figure was in conflict with estimates obtained using a historical approach by groups that ostensibly had access to death counts for September and October from the demographic registry of Puerto Rico, not yet readily available to the general public. By comparing these two numbers to historical averages, additional deaths were estimated to be in excess of 1,000 \cite{santos2018use, robles2017official, rivera2018estimating, pascual2017nearly} raising concerns that Puerto Rico was in the midst of a public health crisis. However, on December 13, 2017, almost three months after landfall, Puerto Ricoâ€™s Secretary of the Department of Public Safety, dismissed these concerns \cite{robles2017official}. As of May 2017 the official death count stood at 64 \cite{hoyos2018puerto} and it appeared the government was not concerned. On May 29, 2018 three days after a study was published describing a survey of 3,299 households and reporting a much higher estimate of the death count received worldwide media coverage \cite{kishore2018mortality}, the government finally made the data public and acknowledged the possibility of a higher excess death count of 1,427 \cite{report2019} based on the vital statistics numbers. On August 2018 the official death toll was revised to 2,975 based on a study commissioned by the government of Puerto Rico \cite{santos2018differential}.

More recently, the limited capacity to perform molecular tests has raised concerns that COVID-19 cases are under-reported in several jurisdictions in the United States and across the world. Simultaneously, polls show that a substantial proportion of the US population believes that reported COVID-19 fatalities are inflated \cite{talev2020}. Estimating excess mortality provides a way to measure the impact of the epidemic, even in the presence of inaccurate reporting.

Here we describe data-driven tools that, when applied to death records, can help answer questions such as: are current excess mortality counts reason for concern? For how long is the population affected by an event?  How much do the effects vary across jurisdictions?  Was there a specific demographic that was at greater risk? What type of medical conditions need the most attention? Our approach provide informative summaries that can help detect concerning patterns, including tools can also help detect abnormalities in close to real-time. 

The main goal of our method is to determine if an observed death count, for a given demographic and period of time is \textit{unusually high}. Reporting unusually high death rates can alert government officials and policy makers that an intervention is required. Because of stochastic variation, \textit{usual} cannot be defined with one number and instead a distribution is appropriate. The uncertainty associated with count data, such as daily deaths totals, are typically accounted for with Poisson models \cite{mccullagh19831989, agresti2015foundations}. However, these models do not account for year-to-year increases or decreases that may be due to natural, possibly correlated in time, stochastic variation. Examples of possible causes are variation in the severity of common colds in the winter or the number of extremely hot days in the summer. The engine behind our tools is a statistical model that accounts for such variability and can be fit to data stratified by demographic group or cause of death. 

To demonstrate the utility of our approach, we use it to provide a detailed picture of the effect Hurricane Mar\'ia had on mortality in Puerto Rico and compare these to those observed in Hugo and Georges, two previous hurricanes in Puerto Rico, Katrina in Louisiana, Irma in Florida, and Sandy in New Jersey. We also investigate the effects of the Chikungunya epidemic in Puerto Rico in 2014 and compare it to estimates from the 2020 spring during the COVID-19 pandemic. We also apply our method to recently released state-level data by the CDC and compare the excess deaths during the COVID-19 epidemic to those from the 2018 flu season. We also compare the excess mortality between racial group in Cook County, Illinois, the only county we are aware of that is currently releasing up to date medical examiner records. Finally, we propose a data-driven index that does not require complete data nor an estimate of population size, which is useful in cases in which estimating population size is challenging thus making death rate estimates less reliable. We make our method available through the \verb+excessmort+ R package and an interactive webtool. \\

\section{Results}
\label{sec:results}

\subsection{Model-based approach provides informative parameterizations}
\label{subsec:model-based}
Following the work developed for monitoring outbreaks \cite{farrington1996statistical,hohle2008count,noufaily2013improved,salmon2016monitoring} we modeled death counts with the following mixed model
\begin{equation}
    Y_t \mid \varepsilon_t \sim 
    \mbox{Poisson}\left\{ \mu_t \right[1 + f(t) \left]  \varepsilon_t \right\} \mbox{ for } t = 1, \dots,T
\end{equation}
with $T$ the total number of observations, $\mu_t$ the expected number of deaths at time $t$ for a typical year, $100 \times f(t)$ the percent increase at time $t$  due to an unusual event, and $\varepsilon_t$ a time series of, possibly auto-correlated, random variables representing natural variability. The expected counts $\mu_t$ can be further decomposed into 
\begin{equation}
    \mu_t =N_t  [\alpha(t) + s(t) + w(t)]
\end{equation}
with $N_t$ the population at time $t$, $\alpha(t)$ a slow moving trend that accounts for changes such as the improved health outcomes we have observed during the last 20 years (Supplemental Figure \ref{supp-fig:trend}), $s(t)$ a yearly periodic function representing a seasonal trend (Supplemental Figure \ref{supp-fig:seasonal}), and $w(t)$ is a day of the week effect. 

We refer to $f$ as the \emph{event effect} and assume $f(t) = 0$ for \emph{typical} periods without a natural disaster or outbreaks.  As described in more details in the Methods Section, we permit non-continuities in $f(t)$ to account for sudden direct effects caused by natural disasters. Note that we can conveniently represent excess deaths at time $t$ as $\mu_t f(t)$ and compute excess deaths for a time period by just adding these up for the $t$'s in the interval of interest. This model introduces two important new improvements that are particularly useful for natural disasters. First, the event effect $f(t)$ can account for both direct and indirect effects. Second, the model accounts for natural variability with correlated errors, an empirically motivated assumption (Supplemental Figure \ref{supp-fig:correlated-errors}) that improves uncertainty estimates as described in \textbf{Section XXXX}. In this section we describe examples of insights provided by our approach and provide details on how we estimate the model parameters and their standard errors in the Methods Section.

\subsection{Detecting and quantifying indirect effects after hurricanes}
\label{subsec:effects}
We fit our model to Puerto Rico mortality data from 1985 to 2020 and found that the death rate in Puerto Rico increased by 72\% (95\% CI: 57\% to 88\%) the days after hurricane Maria made landfall. The increased death rate persisted until well after the end of the year (Figure \ref{fig:fhat-hurricanes}, Supplemental Figure \ref{supp-fig:hurricane-effects}). To understand if such a large and sustained increase is common after an intense hurricane, we examined mortality counts in Louisiana after Katrina, New Jersey after Sandy, Florida after Irma as well as two other hurricanes in Puerto Rico, Hugo in 1989 and Georges in 1998.  Although not as severe, a similar pattern was observed for Georges in 1998 when the death rate increased by 38\% (95\% CI: 24\% to 50\%) the days after landfall and an increased death rate persisted until at least December (Figure \ref{fig:fhat-hurricanes}, Supplemental Figure \ref{supp-fig:hurricane-effects}). The effects of Katrina on Louisiana were much more direct. On August 29, 2005, the day the levees broke, there were 834 deaths, which translates to an increase in death rate of 689\%. However, the increase in mortality rates for the months following this catastrophe was substantially lower than in Puerto Rico after Maria and Georges. None of the other hurricanes examined had noticeable direct or indirect effects (Figure \ref{fig:fhat-hurricanes}, Supplemental Figure \ref{supp-fig:hurricane-effects}).

\begin{figure}[ht]
	\centering
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/figure-1a.pdf} 
		\caption{} 
		\label{fig:fhat-hurricanes}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/figure-1b.pdf}
		\caption{} 
		\label{fig:fhat-chikungunya}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/figure-1c.pdf}
		\caption{} 
		\label{fig:fhat-usa}
	\end{subfigure}
	\caption{}
	\label{fig:fhat-estimates}
\end{figure}

\subsection{Quantifying the effects of epidemics}
\label{subsec:epidemics}
Our model can also be fitted to quantify the effects of epidemics or outbreaks. As an example, 
we fit our model to Puerto Rico mortality data from 2014-2015, when the Chikungunya outbreak occurred \cite{sharp2016surveillance, hsu2019risk}. We detected an increase in mortality rate for a period of 130 days for individuals over 60 years and 167 days for the 0 to 4 years age group (Figure \ref{fig:fhat-chikungunya}). We saw no noticeable effects for other age groups (\ref{supp-fig:chikungunya-agegroups}).

Although mortality data for the COVID-19 pandemic are not yet complete, the CDC has implement a correction to the most recent counts and are making weekly state-level data from January 2017 to May 5, 2020 publicly available \cite{cdc2020covid19}. We fit our model to these data for each state and our event effect estimates clearly detects the COVID-19 pandemic for several states, as well as the 2018 flu epidemics (Figure \ref{fig:fhat-usa}, \textbf{Supplemental Figure XXXX, Table XXXX}). By aggregating the results for the entire United States we confirm that the COVID-19 pandemic had a substantially larger impact than the 2018 season (Figure \ref{fig:}). We also note that the e difference is mostly driven by New York, New Jersey, Massachusetts, Michigan, and Illinois (Supp Figure XXX).

\subsection{Excess mortality estimates}
\label{subsec:excess-mortality}
Once we have fit our model to data, we can compute excess mortality estimates for any time period as well as standard errors that take into account, not just sample variability associated with count processes, but natural variation as well. We used the estimated parameters from the Puerto Rico data to compute excess deaths for every day in a \textbf{XXX} month period after hurricane Georges, Maria made landfall in Puerto Rico and found excess deaths of \textbf{XXX +/- XX} and \textbf{XXX +/- XX} respectively \textbf{(Figure XX)}. We also computed excess death for the \textbf{XX - XX} time period, which our analysis detected as a Chikingunya outbreak, and found an excess mortality of \textbf{XXX-XXX (Figure XX)}. \\ 

Based on the Center for Disease Control and Prevention (CDC) data, we can also compute excess mortality for the period of the COVID-19 epidemic for which we have data, specifically the March 8 to May 2. When we compare these numbers to the reported numbers of COVID-19 deaths, we find a \textbf{XX}\% difference \textbf{(Figure XXX, Table XX)}. We also note that more the US, the number of excess deaths are much larger than the 2018 flu season, although this is not true for all states \textbf{(Figure XX, Table XX)}.

Comparing the data from the New York Times \cite{smith2020coronavirus} we also note the reported COVID-19 are under-counting in many states \textbf{(Table XX)}.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/figure-2a.pdf} 
		\caption{} 
		\label{fig:excess-pr}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/figure-2b.pdf}
		\caption{} 
		\label{fig:excess-usa}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/figure-2c.pdf}
		\caption{} 
		\label{fig:excess-flu-vs-covid}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/figure-2d.pdf}
		\caption{} 
		\label{fig:excess-cook}
	\end{subfigure}
	\caption{}
	\label{fig:excess-deaths}
\end{figure}

\subsection{Across group comparisons}
\label{subsec:group-comparison}
Because our model provides both estimates and standard errors, we can also compare the event effects for two or more groups and assess the statistical significance of the observed differences. The Cook County medical examiner is proving daily updates of their death registry. Although only about \textbf{1/5XXX} of all deaths are examined by this, one of the criteria for an examination from this office is that the death is disease constituting a threat to public health, which implies some if not most COVID-19 deaths are being examined. This database includes demographic information permitting comparisons among, for example, racial groups. The largest percent increases were observed for individuals over 65 \textbf{(Supp Fig X)}. Among those 65 years of age and older, the increase in medical examiner death rate for Cook county was as high as \textbf{XXX} on \textbf{XXXX} and shows a significant difference between black and white populations. The population size for other racial groups of this age was substantially smaller and were not included in the analysis.
 
% disease reached Puerto Rico and our excess death method reveals that in the month of XX there were XXX deaths among people over 60, v is an arbovirus common to tropical regions, mainly in Africa and Asia. According to XXX, most infected individuals recover 10 days, and deaths are rare. In 2014 the disease reached Puerto Rico and our excess death method reveals that in the month of XX there were XXX deaths among people over 60, very likely due to a Chikungunya outbreak. The next XX months saw an incread in the death rate of individuals over 60  (Supplemental Figure 1b) as well as in infants (Supplemental Figure 1b)  with a final excess mortality count of XXXX  +/- XXXX (Supplemental FIgure 1c).

\subsection{Relative proportion of specific causes of death can be used with incomplete data}
\label{subsec:causes-deaths}
Limitations of approaches such as our for detecting and monitoring the possibility of a health crisis are that it requires 1) complete death count data and 2) an estimate of population displacement. Note that incomplete death counts or underestimates of the population displacement result in underestimate of the death rate. If we redefine $Y_t$ as the number of deaths do to a specific cause at time $t$ and $N_t$ as the total number of recorded deaths at time $t$, then the estimate of $f$ can be interpreted as the percent increase in the rate for that cause of death even if the data is incomplete. using this approach we find that deaths due to bacterial infection were elevated during the \textbf{XXX} months following the landfall of hurricane Maria in Puerto RIco. A similar index for respiratory disease computed for the COVID-19 pandemic shows \textbf{XXX (Figure XXX)}.

\subsection{Natural Variability and Correlated Counts}
\label{subsec:natural-variability}
\begin{itemize}
% Need to finish this section
    \item Do previous methods take this into account? If not write a paragraph showing it is necessary
    \item The variance is 0.05
    \item Plot of ACF
\end{itemize}

\section{Methods}
\label{sec:methods}

\subsection{Mortality Data}
\label{subsec:mortality-data}
We obtained detailed mortality and population size data related to hurricanes Hugo, Georges, and Maria in Puerto Rico, Katrina in Louisiana, Sandy in New Jersey, and Irma in Florida. For Puerto Rico, we requested individual level mortality information with no personal identifiers from the Department of Health of Puerto Rico and obtained individual records including date, gender, age, municipality, and cause of death from January 1985 to May \textbf{XX} 2020. We used this data to construct daily death counts for different strata of the Puerto Rican population throughout the study period. We also requested daily death counts from Florida, New Jersey, and Louisianaâ€™s Vital Statistic systems and obtained data from January 2015 to December 2018, January 2007 to December 2015, and January 2003 to December 2006, respectively. Note that we did not ask for individual records from these jurisdictions.

We also obtained estimates of excess deaths made public on May 12, 2020 by the CDC \cite{cdc2020covid19}. This dataset included weekly estimates of death counts for each state and Puerto Rico for the period of January 1, 2017 and May 12, 2020. Furthermore, we obtained individual level mortality data from the Cook County, IL, medical examiner office \textbf{[CITE \href{https://www.cookcountyil.gov/agency/medical-examiner}{cook-county}]}. These data included demographic information for each death including age, sex, and race.

\subsection{Population Estimates}
\label{subsec:population}
Yearly population estimates for Puerto Rico were obtained from the Puerto Rico Statistical Institute. Yearly population estimates for other US states were obtained from the US Census bureau. We computed daily population estimates via linear interpolation of the available data. To obtain an estimate of the population displacement after Hurricane Maria in Puerto Rico, we use population proportion estimates from Teralytics \textbf{(Supplemental Figure XXXX)}.

\subsection{Statistical Model}
\label{subsec:statistical-model}
We assume the mortality counts at $t$ follows a generalized linear mixed model defined above in the Results section. Specifically, we assume:
\begin{equation*}
    Y_t \mid \varepsilon_t \propto \mbox{Poisson}[ \mu_t \{1 + f(t)\}  \varepsilon_t ], \mbox{ for } t = 1, \dots,T 
\end{equation*}
where the expected counts have the form:
\begin{equation*}
    \mu_t =N_t  \exp\{ \alpha(t) + s(t) + w(t) \}
\end{equation*}
and the vector $[\varepsilon_1, \ldots, \varepsilon_T]^\top$ has a multivariate normal distribution with mean 1 and variance-covariance matrix $\Sigma$ determined by an auto-regressive (AR) process of order $p$:
\begin{equation*}
    \Sigma = \sigma^2
    \begin{pmatrix} 1 & \rho_1 & \rho_{2} & \dots & \rho_{T} \\ 
    \rho_{1} & 1 & \rho_{1} & \ldots & \rho_{T-1} \\
    \vdots & \vdots & \vdots & \vdots & \vdots \\
    \rho_{T} & \rho_{T-1} & \rho_{T-2} & \dots & 1 \\ 
    \end{pmatrix}
\end{equation*}
Here $\sigma$ corresponds to the year-to-year standard deviation and $\rho_h$ is the auto-correlation of two error terms that are $h$ time units from each other. In practice, obtaining Maximum Likelihood Estimates for this model is not straight-forward, in particular because the flexibility in $f(t)$ and $\Sigma$ make the model non-identifiable in practice\textbf{[CITE?]}. We now describe a three step approach that works well in practice, as demonstrated both by simulation and empirical validation.

We start by modeling  the yearly trend $\alpha(t)$ with natural cubic splines with a knot every five years. We model the seasonal effect $s(t)$ with a harmonic model with at most 3 harmonics. Finally
\begin{equation*}
    w(t) = \sum{k=1}^7 w_k \times 1_{t \bmod 7  == k} \mbox{ with } \sum_{k=1}^7 = 0
\end{equation*}
represents seven constrained parameters, one for each day of the week. Because natural disasters and outbreaks are rare, we assume that $f(t)$ is 0 for the majority of our dataset. This implies that for the majority of time points
\begin{equation*}
    Y_t \mid \varepsilon_t \propto \mbox{Poisson}[ \mu_t  \varepsilon_t ]
\end{equation*}
This implies that, for example, for $N = T / 365$ years of daily data we would have $T$ data points to fit a model with at most N/5 + 6 + 6 parameters. For five years of data this translates into 1,826 data points and 14 parameters. This implies that we can obtain highly precise estimates even in the presence of the extra dispersion introduced by $\varepsilon$.  We therefore fit a Poisson Generalized Linear Model to regions that we know do not include natural disasters or outbreaks and consider the MLE $\hat{\alpha}(t)$ and $\hat{s}(t)$ as fixed.

In the second step we estimate $\Sigma$, and to do this we select a control region that is continuous for which we know $f(t)=0$. To estimate the $\sigma^2$ and the autoregressive parameters we can use the law of total variance to note that the observed percent change from the mean
\begin{equation*}
    r_t = \frac{Y_t - \mu_t}{\mu_t}
\end{equation*}
has expected value and variance
\begin{equation*}
    \mathbb{E}\left(r_t \right) = 0 \mbox{ and } \mbox{Var}\left(r_t\right)= \sigma^2 + 1/\mu_t
\end{equation*}
Intuitively, these two components are the variance added by $\varepsilon$ and the Poisson variability respectively. The above implies that if we define the transformation:
\begin{equation*}
    Z_t = \frac{r_t}{\sqrt{\mbox{Var}\left(r_t\right)}}
\end{equation*}
we have an approximately normally distributed random variable with the same correlation structure as $\varepsilon_t$, which is what we want to estimate. We use the Yule-Walker equation approach to estimate AR process parameters. To estimate $\sigma^2$ we use:
\begin{equation*}
    \hat{\sigma}^2 = \max\left\{ \frac{1}{T}\sum_{t=1}^T  \left(\frac{Y_t - \hat{\mu}_t}{\hat{\mu}_t}\right)^2  - \frac{1}{\mu_t}, \, 0\right\}
\end{equation*}
Using these estimates we can form an estimate $\hat{\Sigma}$ which we will consider known in the next step.

In the third step we treat this covariance as known, assume $f(t)$ can be modeled as a natural cubic spline, and use iteratively re-weighted least squares to obtain an estimate $\hat{f}(t)$. To account for non-continuities due to direct effects from natural disasters we permit the spline to be discontinuous at the event day. 

\subsection{Approximation for uncorrelated data}
Note that our model can be easily adapted to be fitted to weekly data. The US Census data is weekly for example. In this case there is no need to include a weekday effect parameters nor a correlation structure for the errors. We find that fitting an overdispersed GLM provides practically equivalent results as the more complex GLMM and therefore uses it for weekly data, as well as other datasets in which there does not appear to be correlated counts. 

\subsection{Simulation Study}
To confirm that our estimation procedure produces useful result we conducted a Monte Carlo simulation. Specifically, we generated data from the model above with parameters described in \textbf{Supp Tab XX} and \textbf{Supp Fig XX}. Our procedure produces accurate and precise results for both $f$ and $\Sigma$ \textbf{(Supp Figure)}

We also performed an empirical test by estimating excess deaths and the standard error for period for which no natural disaster occured and no known outbreak. In this case the the estimate divided by its standard error should follow a normal distribution.  We find that the model with correlated errors performs best with the Poisson model greatly underestimating the standard error. Adding a over-dispersion parameter improves the results but still underestimates the variance \textbf{(Supp Figure)}. Providing further evidence that considering correlation is important when analyzing daily data.

\subsection{Excess Death Estimates}
\textbf{START HERE: Need to finish this section}
% We first determined the period of indirect effect for each hurricane. To do this, we defined the interval starting on landfall day, denoted here with $t_0$, until the first day, $t_{ij} > t_0$, for which $f(t_{ij})>0$. Because we do not observe $f(t_{ij})$, and instead obtain an estimate we took the conservative approach of defining the period with the day for which the lower part of a marginal 95\% confidence interval crosses 0: $f(t_{ij})âˆ’1.96\hat(se)\{\hat{f}(t_{ij})\}>0$. We denoted this interval with $I$ and defined the excess deaths by adding the observed deaths minus expected deaths for every day in the interval:
% \begin{equation*}
%     \mbox{Excess deaths} = \sum_{t_{ij} \in I} \left[Y_{ij} âˆ’ N_{ijk}\exp\{g_k(t_{ij})+s_k(j)\}\right]
% \end{equation*}
%  We construct a 95\% confidence interval using the following approximation for the Poisson model:
%  \begin{equation*}
%     \mbox{Var(Excess deaths)} = \sum_{t_{ij} \in I} N_{ijk}\exp\{g_k(t_{ij})+s_k(j)\}
%  \end{equation*}
 
\newpage
\bibliographystyle{unsrt}
\bibliography{ref.bib}

\newpage
\begin{center}
\LARGE{\textbf{Supplemental Material}}
\end{center}

% Supplemental figure 1
\begin{suppfigure}[ht]
	\centering
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/supp-figure-1a.pdf} 
		\caption{} 
		\label{supp-fig:trend}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs//supp-figure-1b.pdf}
		\caption{} 
		\label{supp-fig:seasonal}
	\end{subfigure}
	\caption{}
	\label{supp-fig:model-components}
\end{suppfigure}

% Supplemental figure 2
\begin{suppfigure}[ht]
	\centering
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/supp-figure-2a.pdf} 
		\caption{} 
		\label{supp-fig:qqplot-poisson}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs//supp-figure-2b.pdf}
		\caption{} 
		\label{supp-fig:correlation}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/supp-figure-2c.pdf} 
		\caption{} 
		\label{supp-fig:qqplot-correlated}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs//supp-figure-2d.pdf}
		\caption{} 
		\label{supp-fig:no-correlation}
	\end{subfigure}
	\caption{}
	\label{supp-fig:correlated-errors}
\end{suppfigure}

% Supplemental figure 3
\begin{suppfigure}[ht]
	\centering
	\includegraphics[width=1\linewidth]{figs/supp-figure-3.pdf} 
	\caption{}
	\label{supp-fig:hurricane-effects}
\end{suppfigure}

% Supplemental figure 4
\begin{suppfigure}[ht]
	\centering
	\includegraphics[width=1\linewidth]{figs/supp-figure-4.pdf} 
	\caption{}
	\label{supp-fig:chikungunya-agegroups}
\end{suppfigure}


\end{document}

